{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d0efa805",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports (non-exhaustive)\n",
    "import numpy as np    \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Conv2D, Flatten, MaxPooling2D\n",
    "# from tensorflow.keras.layers import RandomFlip, RandomZoom, RandomRotation, Rescaling\n",
    "from tensorflow.python.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "\n",
    "# .python must be added for tensorflow versions 1.xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4f5ae331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories definition\n",
    "train_path = \"C:/Users/user/Desktop/Chest Xray Classifier/dataset/train\"\n",
    "test_path = \"C:/Users/user/Desktop/Chest Xray Classifier/dataset/test\"\n",
    "val_path = \"C:/Users/user/Desktop/Chest Xray Classifier/dataset/val\"\n",
    "\n",
    "# Basic parameters (image dimension and batch size)\n",
    "batch_size = 16\n",
    "img_height = 500\n",
    "img_width = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7763d0c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4192 files belonging to 2 classes.\n",
      "Found 624 files belonging to 2 classes.\n",
      "Found 1040 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "# Data Preparation\n",
    "\n",
    "# Load data\n",
    "\n",
    "train = tf.keras.utils.image_dataset_from_directory(train_path,\n",
    "                                                    image_size=(img_height, img_width),\n",
    "                                                    color_mode='grayscale',\n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "test = tf.keras.utils.image_dataset_from_directory(test_path,\n",
    "                                                   image_size=(img_height, img_width),\n",
    "                                                   color_mode='grayscale',\n",
    "                                                   shuffle=False, \n",
    "                                                   label_mode='binary',\n",
    "                                                   batch_size=batch_size)\n",
    "\n",
    "valid = tf.keras.utils.image_dataset_from_directory(val_path,\n",
    "                                                    image_size=(img_height, img_width),\n",
    "                                                    color_mode='grayscale',\n",
    "                                                    label_mode='binary',\n",
    "                                                    batch_size=batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "76e4a447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture (Input->CNN->Flat->ANN->Output)\n",
    "cnn = Sequential()\n",
    "\n",
    "# Preprocessing (data augmentation) layers\n",
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.Rescaling(1./255),\n",
    "    tf.keras.layers.RandomFlip(\"horizontal\"),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    \n",
    "    ])\n",
    "cnn.add(data_augmentation)\n",
    "\n",
    "# Convolution and Pooling Layers\n",
    "cnn.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(img_width, img_height, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Conv2D(32, (3, 3), activation=\"relu\", input_shape=(img_width, img_height, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Conv2D(64, (3, 3), activation=\"relu\", input_shape=(img_width, img_height, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "cnn.add(Conv2D(64, (3, 3), activation=\"relu\", input_shape=(img_width, img_height, 1)))\n",
    "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "# Flatten \n",
    "cnn.add(Flatten())\n",
    "\n",
    "# Fully-connected Neural Network Layers\n",
    "cnn.add(Dense(activation='relu', units = 128))\n",
    "cnn.add(Dense(activation='relu', units = 64))\n",
    "cnn.add(Dense(activation='relu', units = 64))\n",
    "cnn.add(Dense(activation='sigmoid', units = 1))\n",
    "\n",
    "cnn.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Regularization callbacks\n",
    "early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=3) # Patience: no. of epochs to run after monitored parameter stops changing\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.3, min_lr=0.00001)\n",
    "callbacks_ls = [early, learning_rate_reduction] \n",
    "\n",
    "# Class weights for unbalanced datasets\n",
    "# Assign higher weights to the minority class, reduce bias towards majority class\n",
    "# Calculate proportion, invert it as a counter-bias\n",
    "# from sklearn.utils.class_weight import compute_class_weight\n",
    "# weights = compute_class_weight('balanced', np.unique(train.class_names))\n",
    "# class_weight_val = dict(zip(np.unique(train.class_names), weights))\n",
    "# #print(class_weight_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2da9a0ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\user\\Desktop\\Chest Xray Classifier\\model.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Chest%20Xray%20Classifier/model.ipynb#W6sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Train model (run this cell to train)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Chest%20Xray%20Classifier/model.ipynb#W6sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# cnn.fit(train, epochs=30, validation_data=valid, class_weight=class_weight_val, callbacks=callbacks_ls)\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Chest%20Xray%20Classifier/model.ipynb#W6sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m cnn\u001b[39m.\u001b[39;49mfit(train, epochs\u001b[39m=\u001b[39;49m\u001b[39m30\u001b[39;49m, validation_data\u001b[39m=\u001b[39;49mvalid, callbacks\u001b[39m=\u001b[39;49mcallbacks_ls)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Chest%20Xray%20Classifier/model.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# Training metric visualization\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/user/Desktop/Chest%20Xray%20Classifier/model.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m pd\u001b[39m.\u001b[39mDataFrame(cnn\u001b[39m.\u001b[39mhistory\u001b[39m.\u001b[39mhistory)\u001b[39m.\u001b[39mplot()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py:1138\u001b[0m, in \u001b[0;36mModel.fit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1132\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_cluster_coordinator \u001b[39m=\u001b[39m cluster_coordinator\u001b[39m.\u001b[39mClusterCoordinator(\n\u001b[0;32m   1133\u001b[0m       \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy)\n\u001b[0;32m   1135\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdistribute_strategy\u001b[39m.\u001b[39mscope(), \\\n\u001b[0;32m   1136\u001b[0m      training_utils\u001b[39m.\u001b[39mRespectCompiledTrainableState(\u001b[39mself\u001b[39m):\n\u001b[0;32m   1137\u001b[0m   \u001b[39m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[39;00m\n\u001b[1;32m-> 1138\u001b[0m   data_handler \u001b[39m=\u001b[39m data_adapter\u001b[39m.\u001b[39;49mget_data_handler(\n\u001b[0;32m   1139\u001b[0m       x\u001b[39m=\u001b[39;49mx,\n\u001b[0;32m   1140\u001b[0m       y\u001b[39m=\u001b[39;49my,\n\u001b[0;32m   1141\u001b[0m       sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1142\u001b[0m       batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1143\u001b[0m       steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1144\u001b[0m       initial_epoch\u001b[39m=\u001b[39;49minitial_epoch,\n\u001b[0;32m   1145\u001b[0m       epochs\u001b[39m=\u001b[39;49mepochs,\n\u001b[0;32m   1146\u001b[0m       shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1147\u001b[0m       class_weight\u001b[39m=\u001b[39;49mclass_weight,\n\u001b[0;32m   1148\u001b[0m       max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1149\u001b[0m       workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1150\u001b[0m       use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1151\u001b[0m       model\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m,\n\u001b[0;32m   1152\u001b[0m       steps_per_execution\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_steps_per_execution)\n\u001b[0;32m   1154\u001b[0m   \u001b[39m# Container that configures and calls `tf.keras.Callback`s.\u001b[39;00m\n\u001b[0;32m   1155\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(callbacks, callbacks_module\u001b[39m.\u001b[39mCallbackList):\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1398\u001b[0m, in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1396\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mgetattr\u001b[39m(kwargs[\u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39m_cluster_coordinator\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m   1397\u001b[0m   \u001b[39mreturn\u001b[39;00m _ClusterCoordinatorDataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m-> 1398\u001b[0m \u001b[39mreturn\u001b[39;00m DataHandler(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1153\u001b[0m, in \u001b[0;36mDataHandler.__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1150\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_steps_per_execution_value \u001b[39m=\u001b[39m steps_per_execution\u001b[39m.\u001b[39mnumpy()\u001b[39m.\u001b[39mitem()\n\u001b[0;32m   1152\u001b[0m adapter_cls \u001b[39m=\u001b[39m select_data_adapter(x, y)\n\u001b[1;32m-> 1153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_adapter \u001b[39m=\u001b[39m adapter_cls(\n\u001b[0;32m   1154\u001b[0m     x,\n\u001b[0;32m   1155\u001b[0m     y,\n\u001b[0;32m   1156\u001b[0m     batch_size\u001b[39m=\u001b[39;49mbatch_size,\n\u001b[0;32m   1157\u001b[0m     steps\u001b[39m=\u001b[39;49msteps_per_epoch,\n\u001b[0;32m   1158\u001b[0m     epochs\u001b[39m=\u001b[39;49mepochs \u001b[39m-\u001b[39;49m initial_epoch,\n\u001b[0;32m   1159\u001b[0m     sample_weights\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m   1160\u001b[0m     shuffle\u001b[39m=\u001b[39;49mshuffle,\n\u001b[0;32m   1161\u001b[0m     max_queue_size\u001b[39m=\u001b[39;49mmax_queue_size,\n\u001b[0;32m   1162\u001b[0m     workers\u001b[39m=\u001b[39;49mworkers,\n\u001b[0;32m   1163\u001b[0m     use_multiprocessing\u001b[39m=\u001b[39;49muse_multiprocessing,\n\u001b[0;32m   1164\u001b[0m     distribution_strategy\u001b[39m=\u001b[39;49mdistribute_lib\u001b[39m.\u001b[39;49mget_strategy(),\n\u001b[0;32m   1165\u001b[0m     model\u001b[39m=\u001b[39;49mmodel)\n\u001b[0;32m   1167\u001b[0m strategy \u001b[39m=\u001b[39m distribute_lib\u001b[39m.\u001b[39mget_strategy()\n\u001b[0;32m   1169\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_current_step \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:723\u001b[0m, in \u001b[0;36mDatasetAdapter.__init__\u001b[1;34m(self, x, y, sample_weights, steps, **kwargs)\u001b[0m\n\u001b[0;32m    720\u001b[0m \u001b[39m# The user-provided steps.\u001b[39;00m\n\u001b[0;32m    721\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_user_steps \u001b[39m=\u001b[39m steps\n\u001b[1;32m--> 723\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_args(y, sample_weights, steps)\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:763\u001b[0m, in \u001b[0;36mDatasetAdapter._validate_args\u001b[1;34m(self, y, sample_weights, steps)\u001b[0m\n\u001b[0;32m    759\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`sample_weight` argument is not supported when using \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    760\u001b[0m                    \u001b[39m\"\u001b[39m\u001b[39mdataset as input.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    762\u001b[0m \u001b[39mif\u001b[39;00m steps \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 763\u001b[0m   \u001b[39mif\u001b[39;00m _is_distributed_dataset(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset):\n\u001b[0;32m    764\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mWhen providing a distributed dataset, you must \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    765\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mspecify the number of steps to run.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    767\u001b[0m   size \u001b[39m=\u001b[39m cardinality\u001b[39m.\u001b[39mcardinality(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset)\u001b[39m.\u001b[39mnumpy()\n",
      "File \u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\data_adapter.py:1699\u001b[0m, in \u001b[0;36m_is_distributed_dataset\u001b[1;34m(ds)\u001b[0m\n\u001b[0;32m   1698\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_is_distributed_dataset\u001b[39m(ds):\n\u001b[1;32m-> 1699\u001b[0m   \u001b[39mreturn\u001b[39;00m \u001b[39misinstance\u001b[39m(ds, input_lib\u001b[39m.\u001b[39;49mDistributedDatasetInterface)\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow.python.distribute.input_lib' has no attribute 'DistributedDatasetInterface'"
     ]
    }
   ],
   "source": [
    "# Train model (run this cell to train)\n",
    "# cnn.fit(train, epochs=30, validation_data=valid, class_weight=class_weight_val, callbacks=callbacks_ls)\n",
    "cnn.fit(train, epochs=30, validation_data=valid, callbacks=callbacks_ls)\n",
    "# Training metric visualization\n",
    "pd.DataFrame(cnn.history.history).plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model evaluation\n",
    "test_model = cnn.evaluate(test)\n",
    "test_accu = test_model[1] * 100\n",
    "print('Test Accuracy: ', test_accu, '%')\n",
    "\n",
    "# Convert prediction output to binary classes (Sigmoid: 0 < output < 1)\n",
    "prediction = cnn.predict(test, verbose=1)\n",
    "prediction_bin = prediction.copy()\n",
    "\n",
    "for i in prediction_bin:\n",
    "    if (i <= 0.5):\n",
    "        prediction_bin[i] = 0\n",
    "    elif (i > 0.5):\n",
    "        prediction_bin[i] = 1\n",
    "\n",
    "# Confusion matrix\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "con_mat = pd.DataFrame(data=confusion_matrix(test.classes, prediction_bin, labels=[0, 1]),\n",
    "                       index=[\"Actual Normal\", \"Actual Pneumonia\"],\n",
    "                       columns=[\"Predicted Normal\", \"Predicted Pneumonia\"])\n",
    "print(classification_report(y_true=test.classes, y_pred=prediction_bin, target_names=['NORMAL','PNEUMONIA']))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
